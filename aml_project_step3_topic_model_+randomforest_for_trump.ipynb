{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bhanugar/Leveraging-Social-Media-Data-for-Political-Insight/blob/main/aml_project_step3_topic_model_%2Brandomforest_for_trump.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\"C:\\Program Files\\python.exe\" -m venv myenv\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "AMTGPh5JKwpH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "\n",
        "# Print the Python version\n",
        "print(sys.version)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mxPqPwiVxhp6",
        "outputId": "91b674a8-6d29-44f2-d2cd-9a999741d425"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3.10.11 (tags/v3.10.11:7d4cc5a, Apr  5 2023, 00:38:17) [MSC v.1929 64 bit (AMD64)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xl_GlBaAxsQi",
        "outputId": "a195a083-224f-4eed-99c1-d40532eab7e2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: emoji in c:\\users\\bgarikip\\myenv\\lib\\site-packages (2.9.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install emoji"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pandas"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZlmDNr6Uxodn",
        "outputId": "b640b8d7-d175-47e5-af6c-031ed3e655a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in c:\\users\\bgarikip\\myenv\\lib\\site-packages (2.1.4)\n",
            "Requirement already satisfied: numpy<2,>=1.22.4 in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from pandas) (1.26.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from pandas) (2023.3.post1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from pandas) (2023.3)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Importing Libraries\n",
        "import pandas as pd\n",
        "import re"
      ],
      "metadata": {
        "id": "rlGsx6gHx2lE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w_po9Ylox0Qv",
        "outputId": "a268d0b7-0ff2-43b1-e8b7-4bc2b30aef24"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in c:\\users\\bgarikip\\myenv\\lib\\site-packages (3.8.1)\n",
            "Requirement already satisfied: click in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from nltk) (2023.10.3)\n",
            "Requirement already satisfied: tqdm in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from nltk) (4.66.1)\n",
            "Requirement already satisfied: colorama in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from click->nltk) (0.4.6)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import nltk\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "import emoji"
      ],
      "metadata": {
        "id": "U9Y62wERx39T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "HhP8tIUtH6Uh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pyarrow"
      ],
      "metadata": {
        "id": "5QdgN1HEyLiE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "82505ba2-7987-4c50-8784-001ebfe616c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyarrow in c:\\users\\bgarikip\\myenv\\lib\\site-packages (14.0.1)\n",
            "Requirement already satisfied: numpy>=1.16.6 in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from pyarrow) (1.26.2)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#file_path = 'Downloads/df_all_merged&filtered_cleaned_data_topiclabels.feather'\n",
        "file_path2 = 'Downloads/df_all_sentiment.feather'\n",
        "\n",
        "df = pd.read_feather(file_path2)\n",
        "#dfmar = pd.read_feather(file_path2)"
      ],
      "metadata": {
        "id": "hSGOQBYEx56y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "id": "TYIPrwrs9HS2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Analyzing the relationship between 'dominant_topic' and 'retweet_count'\n",
        "topic_retweet_analysis = test_df.groupby('sentiment')['retweet_count'].mean().sort_values(ascending=False)\n",
        "\n",
        "# Plotting the analysis\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(x=topic_retweet_analysis.index, y=topic_retweet_analysis.values)\n",
        "plt.title('Average Retweet Count by sentiment  ')\n",
        "plt.xlabel('sentiment')\n",
        "plt.ylabel('Average Retweet Count')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Dyw982TB9AzM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "def categorize_tweets_by_userid(df, userid_column):\n",
        "\n",
        "    # Define the conditions for categorization\n",
        "    conditions = [\n",
        "        df[userid_column] == '25073877',  # Trump's user ID\n",
        "        df[userid_column] == '1339835893'  # Clinton's user ID\n",
        "    ]\n",
        "\n",
        "    # Define the corresponding labels for the conditions\n",
        "    choices = ['trump', 'clinton']\n",
        "\n",
        "    # Apply the conditions and choices to the DataFrame\n",
        "    df['tweet_by'] = np.select(conditions, choices, default='others')\n",
        "\n",
        "    return df\n",
        "\n",
        "# Apply the function to your DataFrame\n",
        "df = categorize_tweets_by_userid(df, 'userid')\n"
      ],
      "metadata": {
        "id": "H1kK1I5SyPMS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filtered_dft = df[df['tweet_by'] == 'trump']\n",
        "filtered_dfc = df[df['tweet_by'] == 'clinton']\n",
        "filtered_dfo = df[df['tweet_by'] == 'others']"
      ],
      "metadata": {
        "id": "o6ZkCiDfA-im"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sampled_df = filtered_dfo.sample(frac=0.17, random_state=42)\n"
      ],
      "metadata": {
        "id": "3AFRruGl-eVV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_merged = pd.concat([filtered_dft, filtered_dfc, sampled_df], ignore_index=True)\n"
      ],
      "metadata": {
        "id": "1OpGJhbDBYAu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unique_userid_count = df_merged['userid'].nunique()\n",
        "print(unique_userid_count)\n",
        "# there are 852278 unique users in this dataset."
      ],
      "metadata": {
        "id": "OFeuFJ8qyEB8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1140425c-ed95-4e05-aac5-33758376d299"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1056708\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(df_merged)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DshKVoX4CHWE",
        "outputId": "930e8968-77ca-4556-f0e9-f626b77e8df8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3917432"
            ]
          },
          "metadata": {},
          "execution_count": 614
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(df_merged[df_merged['tweet_by'] == 'others'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OoLuFI6GBsuM",
        "outputId": "1e9ffe97-3e7c-4675-cbec-aa15b6b78292"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3909740"
            ]
          },
          "metadata": {},
          "execution_count": 615
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install scikit-learn"
      ],
      "metadata": {
        "id": "mJVszZ5Kyev1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8599e59b-9807-4fdb-9c0f-0db085238861"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scikit-learn in c:\\users\\bgarikip\\myenv\\lib\\site-packages (1.3.2)Note: you may need to restart the kernel to use updated packages.\n",
            "\n",
            "Requirement already satisfied: numpy<2.0,>=1.17.3 in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from scikit-learn) (1.26.2)\n",
            "Requirement already satisfied: scipy>=1.5.0 in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from scikit-learn) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from scikit-learn) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from scikit-learn) (3.2.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_merged = df_merged.reset_index(drop=True)"
      ],
      "metadata": {
        "id": "0lSja4jDCZLL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "top2vec is used for the topic modeling but it alos taking to analyize longer than we expected so to finish this in given time we had to consider a smaller dataset. so we took all the tweets by the trump and used that filtered dataset for next process. if we got more resources we could gone without any filtration."
      ],
      "metadata": {
        "id": "vg23K285EjUV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Assuming df_final_2 is your DataFrame\n",
        "train_df, test_df = train_test_split(df_merged, test_size=0.1, random_state=42)\n",
        "# Now train_df contains 90% of the data, and test_df contains 10% of the data\n",
        "train_df = train_df.reset_index(drop=True)\n",
        "test_df = test_df          .reset_index(drop=True)"
      ],
      "metadata": {
        "id": "NpM-iJiryh7Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vngXoLU4Pq3O",
        "outputId": "2494cbd8-c82f-4e0b-9f4e-976dca090bff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3525688"
            ]
          },
          "metadata": {},
          "execution_count": 619
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(test_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bsKBeUADPsh0",
        "outputId": "830e658c-e04e-4e77-ab09-73bf3512cad3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3525688"
            ]
          },
          "metadata": {},
          "execution_count": 620
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow\n",
        "!pip install tensorflow-hub\n",
        "!pip install tensorflow-text\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Zfjr-RcyCDs",
        "outputId": "ae6eedf6-6c54-41cb-9bf6-d80ed6108e93"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in c:\\users\\bgarikip\\myenv\\lib\\site-packages (2.10.1)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from tensorflow) (2.0.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from tensorflow) (23.5.26)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from tensorflow) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from tensorflow) (3.10.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from tensorflow) (1.1.2)\n",
            "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from tensorflow) (16.0.6)\n",
            "Requirement already satisfied: numpy>=1.20 in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from tensorflow) (1.26.2)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from tensorflow) (23.2)\n",
            "Requirement already satisfied: protobuf<3.20,>=3.9.2 in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from tensorflow) (3.19.6)\n",
            "Requirement already satisfied: setuptools in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from tensorflow) (65.5.0)\n",
            "Requirement already satisfied: six>=1.12.0 in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from tensorflow) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from tensorflow) (4.9.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from tensorflow) (0.31.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from tensorflow) (1.60.0)\n",
            "Requirement already satisfied: tensorboard<2.11,>=2.10 in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from tensorflow) (2.10.1)\n",
            "Requirement already satisfied: tensorflow-estimator<2.11,>=2.10.0 in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from tensorflow) (2.10.0)\n",
            "Requirement already satisfied: keras<2.11,>=2.10.0 in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from tensorflow) (2.10.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from astunparse>=1.6.0->tensorflow) (0.42.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow) (2.25.2)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow) (0.4.6)\n",
            "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow) (3.5.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow) (0.6.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow) (1.8.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow) (3.0.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow) (5.3.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.11,>=2.10->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow) (2.1.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow) (2023.11.17)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.11,>=2.10->tensorflow) (2.1.3)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow) (0.5.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.11,>=2.10->tensorflow) (3.2.2)\n",
            "Requirement already satisfied: tensorflow-hub in c:\\users\\bgarikip\\myenv\\lib\\site-packages (0.15.0)\n",
            "Requirement already satisfied: numpy>=1.12.0 in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from tensorflow-hub) (1.26.2)\n",
            "Requirement already satisfied: protobuf>=3.19.6 in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from tensorflow-hub) (3.19.6)\n",
            "Requirement already satisfied: tensorflow-text in c:\\users\\bgarikip\\myenv\\lib\\site-packages (2.10.0)\n",
            "Requirement already satisfied: tensorflow-hub>=0.8.0 in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from tensorflow-text) (0.15.0)\n",
            "Requirement already satisfied: tensorflow<2.11,>=2.10.0 in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from tensorflow-text) (2.10.1)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from tensorflow<2.11,>=2.10.0->tensorflow-text) (2.0.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from tensorflow<2.11,>=2.10.0->tensorflow-text) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from tensorflow<2.11,>=2.10.0->tensorflow-text) (23.5.26)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from tensorflow<2.11,>=2.10.0->tensorflow-text) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from tensorflow<2.11,>=2.10.0->tensorflow-text) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from tensorflow<2.11,>=2.10.0->tensorflow-text) (3.10.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from tensorflow<2.11,>=2.10.0->tensorflow-text) (1.1.2)\n",
            "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from tensorflow<2.11,>=2.10.0->tensorflow-text) (16.0.6)\n",
            "Requirement already satisfied: numpy>=1.20 in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from tensorflow<2.11,>=2.10.0->tensorflow-text) (1.26.2)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from tensorflow<2.11,>=2.10.0->tensorflow-text) (3.3.0)\n",
            "Requirement already satisfied: packaging in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from tensorflow<2.11,>=2.10.0->tensorflow-text) (23.2)\n",
            "Requirement already satisfied: protobuf<3.20,>=3.9.2 in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from tensorflow<2.11,>=2.10.0->tensorflow-text) (3.19.6)\n",
            "Requirement already satisfied: setuptools in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from tensorflow<2.11,>=2.10.0->tensorflow-text) (65.5.0)\n",
            "Requirement already satisfied: six>=1.12.0 in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from tensorflow<2.11,>=2.10.0->tensorflow-text) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from tensorflow<2.11,>=2.10.0->tensorflow-text) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from tensorflow<2.11,>=2.10.0->tensorflow-text) (4.9.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from tensorflow<2.11,>=2.10.0->tensorflow-text) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from tensorflow<2.11,>=2.10.0->tensorflow-text) (0.31.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from tensorflow<2.11,>=2.10.0->tensorflow-text) (1.60.0)\n",
            "Requirement already satisfied: tensorboard<2.11,>=2.10 in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from tensorflow<2.11,>=2.10.0->tensorflow-text) (2.10.1)\n",
            "Requirement already satisfied: tensorflow-estimator<2.11,>=2.10.0 in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from tensorflow<2.11,>=2.10.0->tensorflow-text) (2.10.0)\n",
            "Requirement already satisfied: keras<2.11,>=2.10.0 in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from tensorflow<2.11,>=2.10.0->tensorflow-text) (2.10.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from astunparse>=1.6.0->tensorflow<2.11,>=2.10.0->tensorflow-text) (0.42.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow<2.11,>=2.10.0->tensorflow-text) (2.25.2)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow<2.11,>=2.10.0->tensorflow-text) (0.4.6)\n",
            "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow<2.11,>=2.10.0->tensorflow-text) (3.5.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow<2.11,>=2.10.0->tensorflow-text) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow<2.11,>=2.10.0->tensorflow-text) (0.6.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow<2.11,>=2.10.0->tensorflow-text) (1.8.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow<2.11,>=2.10.0->tensorflow-text) (3.0.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow<2.11,>=2.10.0->tensorflow-text) (5.3.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow<2.11,>=2.10.0->tensorflow-text) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow<2.11,>=2.10.0->tensorflow-text) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.11,>=2.10->tensorflow<2.11,>=2.10.0->tensorflow-text) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow<2.11,>=2.10.0->tensorflow-text) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow<2.11,>=2.10.0->tensorflow-text) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow<2.11,>=2.10.0->tensorflow-text) (2.1.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow<2.11,>=2.10.0->tensorflow-text) (2023.11.17)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.11,>=2.10->tensorflow<2.11,>=2.10.0->tensorflow-text) (2.1.3)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow<2.11,>=2.10.0->tensorflow-text) (0.5.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.11,>=2.10->tensorflow<2.11,>=2.10.0->tensorflow-text) (3.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install top2vec\n"
      ],
      "metadata": {
        "id": "Z9Z22JusaIFa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b9ced71f-53e9-4791-ec24-d7b467a273eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: top2vec in c:\\users\\bgarikip\\myenv\\lib\\site-packages (1.0.34)Note: you may need to restart the kernel to use updated packages.\n",
            "\n",
            "Requirement already satisfied: numpy>=1.20.0 in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from top2vec) (1.26.2)\n",
            "Requirement already satisfied: pandas in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from top2vec) (2.1.4)\n",
            "Requirement already satisfied: scikit-learn>=1.2.0 in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from top2vec) (1.3.2)\n",
            "Requirement already satisfied: gensim>=4.0.0 in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from top2vec) (4.3.2)\n",
            "Requirement already satisfied: umap-learn>=0.5.1 in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from top2vec) (0.5.5)\n",
            "Requirement already satisfied: hdbscan>=0.8.27 in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from top2vec) (0.8.33)\n",
            "Requirement already satisfied: wordcloud in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from top2vec) (1.9.3)\n",
            "Requirement already satisfied: scipy>=1.7.0 in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from gensim>=4.0.0->top2vec) (1.11.4)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from gensim>=4.0.0->top2vec) (6.4.0)\n",
            "Requirement already satisfied: cython<3,>=0.27 in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from hdbscan>=0.8.27->top2vec) (0.29.36)\n",
            "Requirement already satisfied: joblib>=1.0 in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from hdbscan>=0.8.27->top2vec) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from scikit-learn>=1.2.0->top2vec) (3.2.0)\n",
            "Requirement already satisfied: numba>=0.51.2 in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from umap-learn>=0.5.1->top2vec) (0.58.1)\n",
            "Requirement already satisfied: pynndescent>=0.5 in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from umap-learn>=0.5.1->top2vec) (0.5.11)\n",
            "Requirement already satisfied: tqdm in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from umap-learn>=0.5.1->top2vec) (4.66.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from pandas->top2vec) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from pandas->top2vec) (2023.3.post1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from pandas->top2vec) (2023.3)\n",
            "Requirement already satisfied: pillow in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from wordcloud->top2vec) (10.1.0)\n",
            "Requirement already satisfied: matplotlib in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from wordcloud->top2vec) (3.8.2)\n",
            "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from numba>=0.51.2->umap-learn>=0.5.1->top2vec) (0.41.1)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->top2vec) (1.16.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from matplotlib->wordcloud->top2vec) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from matplotlib->wordcloud->top2vec) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from matplotlib->wordcloud->top2vec) (4.46.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from matplotlib->wordcloud->top2vec) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from matplotlib->wordcloud->top2vec) (23.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from matplotlib->wordcloud->top2vec) (3.1.1)\n",
            "Requirement already satisfied: colorama in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from tqdm->umap-learn>=0.5.1->top2vec) (0.4.6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install top2vec[sentence_transformers]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9FTbbwDKabr9",
        "outputId": "ab61d6f1-3837-4bff-d813-3f8fe19bbddd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: top2vec[sentence_transformers] in c:\\users\\bgarikip\\myenv\\lib\\site-packages (1.0.34)\n",
            "Requirement already satisfied: numpy>=1.20.0 in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from top2vec[sentence_transformers]) (1.26.2)\n",
            "Requirement already satisfied: pandas in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from top2vec[sentence_transformers]) (2.1.4)\n",
            "Requirement already satisfied: scikit-learn>=1.2.0 in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from top2vec[sentence_transformers]) (1.3.2)\n",
            "Requirement already satisfied: gensim>=4.0.0 in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from top2vec[sentence_transformers]) (4.3.2)\n",
            "Requirement already satisfied: umap-learn>=0.5.1 in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from top2vec[sentence_transformers]) (0.5.5)\n",
            "Requirement already satisfied: hdbscan>=0.8.27 in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from top2vec[sentence_transformers]) (0.8.33)\n",
            "Requirement already satisfied: wordcloud in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from top2vec[sentence_transformers]) (1.9.3)\n",
            "Requirement already satisfied: torch in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from top2vec[sentence_transformers]) (2.1.2)\n",
            "Requirement already satisfied: sentence-transformers in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from top2vec[sentence_transformers]) (2.2.2)\n",
            "Requirement already satisfied: scipy>=1.7.0 in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from gensim>=4.0.0->top2vec[sentence_transformers]) (1.11.4)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from gensim>=4.0.0->top2vec[sentence_transformers]) (6.4.0)\n",
            "Requirement already satisfied: cython<3,>=0.27 in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from hdbscan>=0.8.27->top2vec[sentence_transformers]) (0.29.36)\n",
            "Requirement already satisfied: joblib>=1.0 in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from hdbscan>=0.8.27->top2vec[sentence_transformers]) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from scikit-learn>=1.2.0->top2vec[sentence_transformers]) (3.2.0)\n",
            "Requirement already satisfied: numba>=0.51.2 in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from umap-learn>=0.5.1->top2vec[sentence_transformers]) (0.58.1)\n",
            "Requirement already satisfied: pynndescent>=0.5 in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from umap-learn>=0.5.1->top2vec[sentence_transformers]) (0.5.11)\n",
            "Requirement already satisfied: tqdm in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from umap-learn>=0.5.1->top2vec[sentence_transformers]) (4.66.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from pandas->top2vec[sentence_transformers]) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from pandas->top2vec[sentence_transformers]) (2023.3.post1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from pandas->top2vec[sentence_transformers]) (2023.3)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from sentence-transformers->top2vec[sentence_transformers]) (4.36.1)\n",
            "Requirement already satisfied: torchvision in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from sentence-transformers->top2vec[sentence_transformers]) (0.16.2)\n",
            "Requirement already satisfied: nltk in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from sentence-transformers->top2vec[sentence_transformers]) (3.8.1)\n",
            "Requirement already satisfied: sentencepiece in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from sentence-transformers->top2vec[sentence_transformers]) (0.1.99)\n",
            "Requirement already satisfied: huggingface-hub>=0.4.0 in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from sentence-transformers->top2vec[sentence_transformers]) (0.19.4)\n",
            "Requirement already satisfied: filelock in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from torch->top2vec[sentence_transformers]) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from torch->top2vec[sentence_transformers]) (4.9.0)\n",
            "Requirement already satisfied: sympy in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from torch->top2vec[sentence_transformers]) (1.12)\n",
            "Requirement already satisfied: networkx in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from torch->top2vec[sentence_transformers]) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from torch->top2vec[sentence_transformers]) (3.1.2)\n",
            "Requirement already satisfied: fsspec in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from torch->top2vec[sentence_transformers]) (2023.12.2)\n",
            "Requirement already satisfied: pillow in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from wordcloud->top2vec[sentence_transformers]) (10.1.0)\n",
            "Requirement already satisfied: matplotlib in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from wordcloud->top2vec[sentence_transformers]) (3.8.2)\n",
            "Requirement already satisfied: requests in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence-transformers->top2vec[sentence_transformers]) (2.31.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence-transformers->top2vec[sentence_transformers]) (6.0.1)\n",
            "Requirement already satisfied: packaging>=20.9 in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence-transformers->top2vec[sentence_transformers]) (23.2)\n",
            "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from numba>=0.51.2->umap-learn>=0.5.1->top2vec[sentence_transformers]) (0.41.1)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->top2vec[sentence_transformers]) (1.16.0)\n",
            "Requirement already satisfied: colorama in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from tqdm->umap-learn>=0.5.1->top2vec[sentence_transformers]) (0.4.6)\n",
            "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers->top2vec[sentence_transformers]) (2023.10.3)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers->top2vec[sentence_transformers]) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers->top2vec[sentence_transformers]) (0.4.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from jinja2->torch->top2vec[sentence_transformers]) (2.1.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from matplotlib->wordcloud->top2vec[sentence_transformers]) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from matplotlib->wordcloud->top2vec[sentence_transformers]) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from matplotlib->wordcloud->top2vec[sentence_transformers]) (4.46.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from matplotlib->wordcloud->top2vec[sentence_transformers]) (1.4.5)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from matplotlib->wordcloud->top2vec[sentence_transformers]) (3.1.1)\n",
            "Requirement already satisfied: click in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from nltk->sentence-transformers->top2vec[sentence_transformers]) (8.1.7)\n",
            "Requirement already satisfied: mpmath>=0.19 in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from sympy->torch->top2vec[sentence_transformers]) (1.3.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers->top2vec[sentence_transformers]) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers->top2vec[sentence_transformers]) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers->top2vec[sentence_transformers]) (2.1.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers->top2vec[sentence_transformers]) (2023.11.17)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING: top2vec 1.0.34 does not provide the extra 'sentence-transformers'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install top2vec[indexing]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OhHjmLB2atmC",
        "outputId": "1f15c7d0-5aee-4778-993b-8a38d2653306"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: top2vec[indexing] in c:\\users\\bgarikip\\myenv\\lib\\site-packages (1.0.34)\n",
            "Requirement already satisfied: numpy>=1.20.0 in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from top2vec[indexing]) (1.26.2)\n",
            "Requirement already satisfied: pandas in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from top2vec[indexing]) (2.1.4)\n",
            "Requirement already satisfied: scikit-learn>=1.2.0 in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from top2vec[indexing]) (1.3.2)\n",
            "Requirement already satisfied: gensim>=4.0.0 in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from top2vec[indexing]) (4.3.2)\n",
            "Requirement already satisfied: umap-learn>=0.5.1 in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from top2vec[indexing]) (0.5.5)\n",
            "Requirement already satisfied: hdbscan>=0.8.27 in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from top2vec[indexing]) (0.8.33)\n",
            "Requirement already satisfied: wordcloud in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from top2vec[indexing]) (1.9.3)\n",
            "Requirement already satisfied: hnswlib in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from top2vec[indexing]) (0.8.0)\n",
            "Requirement already satisfied: scipy>=1.7.0 in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from gensim>=4.0.0->top2vec[indexing]) (1.11.4)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from gensim>=4.0.0->top2vec[indexing]) (6.4.0)\n",
            "Requirement already satisfied: cython<3,>=0.27 in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from hdbscan>=0.8.27->top2vec[indexing]) (0.29.36)\n",
            "Requirement already satisfied: joblib>=1.0 in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from hdbscan>=0.8.27->top2vec[indexing]) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from scikit-learn>=1.2.0->top2vec[indexing]) (3.2.0)\n",
            "Requirement already satisfied: numba>=0.51.2 in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from umap-learn>=0.5.1->top2vec[indexing]) (0.58.1)\n",
            "Requirement already satisfied: pynndescent>=0.5 in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from umap-learn>=0.5.1->top2vec[indexing]) (0.5.11)\n",
            "Requirement already satisfied: tqdm in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from umap-learn>=0.5.1->top2vec[indexing]) (4.66.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from pandas->top2vec[indexing]) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from pandas->top2vec[indexing]) (2023.3.post1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from pandas->top2vec[indexing]) (2023.3)\n",
            "Requirement already satisfied: pillow in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from wordcloud->top2vec[indexing]) (10.1.0)\n",
            "Requirement already satisfied: matplotlib in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from wordcloud->top2vec[indexing]) (3.8.2)\n",
            "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from numba>=0.51.2->umap-learn>=0.5.1->top2vec[indexing]) (0.41.1)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->top2vec[indexing]) (1.16.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from matplotlib->wordcloud->top2vec[indexing]) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from matplotlib->wordcloud->top2vec[indexing]) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from matplotlib->wordcloud->top2vec[indexing]) (4.46.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from matplotlib->wordcloud->top2vec[indexing]) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from matplotlib->wordcloud->top2vec[indexing]) (23.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from matplotlib->wordcloud->top2vec[indexing]) (3.1.1)\n",
            "Requirement already satisfied: colorama in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from tqdm->umap-learn>=0.5.1->top2vec[indexing]) (0.4.6)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install top2vec[sentence_encoders]\n"
      ],
      "metadata": {
        "id": "OLH2rtdbyFBz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b3ac153-faa5-4500-b34a-53c4fd33972f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: top2vec[sentence_encoders] in c:\\users\\bgarikip\\myenv\\lib\\site-packages (1.0.34)\n",
            "Requirement already satisfied: numpy>=1.20.0 in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from top2vec[sentence_encoders]) (1.26.2)\n",
            "Requirement already satisfied: pandas in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from top2vec[sentence_encoders]) (2.1.4)\n",
            "Requirement already satisfied: scikit-learn>=1.2.0 in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from top2vec[sentence_encoders]) (1.3.2)\n",
            "Requirement already satisfied: gensim>=4.0.0 in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from top2vec[sentence_encoders]) (4.3.2)\n",
            "Requirement already satisfied: umap-learn>=0.5.1 in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from top2vec[sentence_encoders]) (0.5.5)\n",
            "Requirement already satisfied: hdbscan>=0.8.27 in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from top2vec[sentence_encoders]) (0.8.33)\n",
            "Requirement already satisfied: wordcloud in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from top2vec[sentence_encoders]) (1.9.3)\n",
            "Requirement already satisfied: tensorflow in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from top2vec[sentence_encoders]) (2.10.1)\n",
            "Requirement already satisfied: tensorflow-hub in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from top2vec[sentence_encoders]) (0.15.0)\n",
            "Requirement already satisfied: tensorflow-text in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from top2vec[sentence_encoders]) (2.10.0)\n",
            "Requirement already satisfied: scipy>=1.7.0 in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from gensim>=4.0.0->top2vec[sentence_encoders]) (1.11.4)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from gensim>=4.0.0->top2vec[sentence_encoders]) (6.4.0)\n",
            "Requirement already satisfied: cython<3,>=0.27 in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from hdbscan>=0.8.27->top2vec[sentence_encoders]) (0.29.36)\n",
            "Requirement already satisfied: joblib>=1.0 in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from hdbscan>=0.8.27->top2vec[sentence_encoders]) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from scikit-learn>=1.2.0->top2vec[sentence_encoders]) (3.2.0)\n",
            "Requirement already satisfied: numba>=0.51.2 in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from umap-learn>=0.5.1->top2vec[sentence_encoders]) (0.58.1)\n",
            "Requirement already satisfied: pynndescent>=0.5 in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from umap-learn>=0.5.1->top2vec[sentence_encoders]) (0.5.11)\n",
            "Requirement already satisfied: tqdm in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from umap-learn>=0.5.1->top2vec[sentence_encoders]) (4.66.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from pandas->top2vec[sentence_encoders]) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from pandas->top2vec[sentence_encoders]) (2023.3.post1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from pandas->top2vec[sentence_encoders]) (2023.3)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from tensorflow->top2vec[sentence_encoders]) (2.0.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from tensorflow->top2vec[sentence_encoders]) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from tensorflow->top2vec[sentence_encoders]) (23.5.26)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from tensorflow->top2vec[sentence_encoders]) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from tensorflow->top2vec[sentence_encoders]) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from tensorflow->top2vec[sentence_encoders]) (3.10.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from tensorflow->top2vec[sentence_encoders]) (1.1.2)\n",
            "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from tensorflow->top2vec[sentence_encoders]) (16.0.6)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from tensorflow->top2vec[sentence_encoders]) (3.3.0)\n",
            "Requirement already satisfied: packaging in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from tensorflow->top2vec[sentence_encoders]) (23.2)\n",
            "Requirement already satisfied: protobuf<3.20,>=3.9.2 in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from tensorflow->top2vec[sentence_encoders]) (3.19.6)\n",
            "Requirement already satisfied: setuptools in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from tensorflow->top2vec[sentence_encoders]) (65.5.0)\n",
            "Requirement already satisfied: six>=1.12.0 in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from tensorflow->top2vec[sentence_encoders]) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from tensorflow->top2vec[sentence_encoders]) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from tensorflow->top2vec[sentence_encoders]) (4.9.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from tensorflow->top2vec[sentence_encoders]) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from tensorflow->top2vec[sentence_encoders]) (0.31.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from tensorflow->top2vec[sentence_encoders]) (1.60.0)\n",
            "Requirement already satisfied: tensorboard<2.11,>=2.10 in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from tensorflow->top2vec[sentence_encoders]) (2.10.1)\n",
            "Requirement already satisfied: tensorflow-estimator<2.11,>=2.10.0 in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from tensorflow->top2vec[sentence_encoders]) (2.10.0)\n",
            "Requirement already satisfied: keras<2.11,>=2.10.0 in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from tensorflow->top2vec[sentence_encoders]) (2.10.0)\n",
            "Requirement already satisfied: pillow in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from wordcloud->top2vec[sentence_encoders]) (10.1.0)\n",
            "Requirement already satisfied: matplotlib in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from wordcloud->top2vec[sentence_encoders]) (3.8.2)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from astunparse>=1.6.0->tensorflow->top2vec[sentence_encoders]) (0.42.0)\n",
            "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from numba>=0.51.2->umap-learn>=0.5.1->top2vec[sentence_encoders]) (0.41.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow->top2vec[sentence_encoders]) (2.25.2)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow->top2vec[sentence_encoders]) (0.4.6)\n",
            "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow->top2vec[sentence_encoders]) (3.5.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow->top2vec[sentence_encoders]) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow->top2vec[sentence_encoders]) (0.6.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow->top2vec[sentence_encoders]) (1.8.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow->top2vec[sentence_encoders]) (3.0.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from matplotlib->wordcloud->top2vec[sentence_encoders]) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from matplotlib->wordcloud->top2vec[sentence_encoders]) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from matplotlib->wordcloud->top2vec[sentence_encoders]) (4.46.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from matplotlib->wordcloud->top2vec[sentence_encoders]) (1.4.5)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from matplotlib->wordcloud->top2vec[sentence_encoders]) (3.1.1)\n",
            "Requirement already satisfied: colorama in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from tqdm->umap-learn>=0.5.1->top2vec[sentence_encoders]) (0.4.6)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow->top2vec[sentence_encoders]) (5.3.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow->top2vec[sentence_encoders]) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow->top2vec[sentence_encoders]) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.11,>=2.10->tensorflow->top2vec[sentence_encoders]) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow->top2vec[sentence_encoders]) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow->top2vec[sentence_encoders]) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow->top2vec[sentence_encoders]) (2.1.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow->top2vec[sentence_encoders]) (2023.11.17)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.11,>=2.10->tensorflow->top2vec[sentence_encoders]) (2.1.3)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow->top2vec[sentence_encoders]) (0.5.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.11,>=2.10->tensorflow->top2vec[sentence_encoders]) (3.2.2)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING: top2vec 1.0.34 does not provide the extra 'sentence-encoders'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install ipywidgets"
      ],
      "metadata": {
        "id": "3UZNN9g7Q7nr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d18dc6d1-8123-4351-df98-663960df4236"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: ipywidgets in c:\\users\\bgarikip\\myenv\\lib\\site-packages (8.1.1)\n",
            "Requirement already satisfied: comm>=0.1.3 in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from ipywidgets) (0.2.0)\n",
            "Requirement already satisfied: ipython>=6.1.0 in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from ipywidgets) (8.18.1)\n",
            "Requirement already satisfied: traitlets>=4.3.1 in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from ipywidgets) (5.14.0)\n",
            "Requirement already satisfied: widgetsnbextension~=4.0.9 in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from ipywidgets) (4.0.9)\n",
            "Requirement already satisfied: jupyterlab-widgets~=3.0.9 in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from ipywidgets) (3.0.9)\n",
            "Requirement already satisfied: decorator in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (5.1.1)\n",
            "Requirement already satisfied: jedi>=0.16 in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.19.1)\n",
            "Requirement already satisfied: matplotlib-inline in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.1.6)\n",
            "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (3.0.43)\n",
            "Requirement already satisfied: pygments>=2.4.0 in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (2.17.2)\n",
            "Requirement already satisfied: stack-data in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.6.3)\n",
            "Requirement already satisfied: exceptiongroup in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (1.2.0)\n",
            "Requirement already satisfied: colorama in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.4.6)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.3 in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.3)\n",
            "Requirement already satisfied: wcwidth in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.12)\n",
            "Requirement already satisfied: executing>=1.2.0 in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.0.1)\n",
            "Requirement already satisfied: asttokens>=2.1.0 in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.4.1)\n",
            "Requirement already satisfied: pure-eval in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.2.2)\n",
            "Requirement already satisfied: six>=1.12.0 in c:\\users\\bgarikip\\myenv\\lib\\site-packages (from asttokens>=2.1.0->stack-data->ipython>=6.1.0->ipywidgets) (1.16.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from top2vec import Top2Vec\n",
        "\n",
        "# Number of CPU cores\n",
        "num_cores = os.cpu_count()\n",
        "\n",
        "print(num_cores)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U96E-2v5o2Yh",
        "outputId": "3d47dcd0-98e8-40be-bcb6-773b6eab2164"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "16\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from top2vec import Top2Vec\n",
        "# Extracting the 'clean_data' column\n",
        "text_data = train_df['clean_data'].dropna().tolist()\n",
        "\n",
        "# Create the Top2Vec model\n",
        "modeltop2vec = Top2Vec(documents=text_data, speed='learn', workers=num_cores)"
      ],
      "metadata": {
        "id": "2hUDZ2noyjbU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "03cb9580-4c21-41e4-cddb-8faab70f1a54"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2023-12-20 16:10:22,858 - top2vec - INFO - Pre-processing documents for training\n",
            "INFO:top2vec:Pre-processing documents for training\n",
            "C:\\Users\\bgarikip\\myenv\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n",
            "2023-12-20 16:12:12,991 - top2vec - INFO - Downloading universal-sentence-encoder-multilingual model\n",
            "INFO:top2vec:Downloading universal-sentence-encoder-multilingual model\n",
            "2023-12-20 16:12:15,255 - top2vec - INFO - Creating joint document/word embedding\n",
            "INFO:top2vec:Creating joint document/word embedding\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "modeltop2vec.get_num_topics()"
      ],
      "metadata": {
        "id": "lblscrN3ynki"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "topic_sizes , topic_nums = modeltop2vec.get_topic_sizes()\n",
        "print(topic_nums)"
      ],
      "metadata": {
        "id": "tN02k-DqyrLa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "topic_words,word_scores,topic_nums = modeltop2vec.get_topics(3)\n",
        "for words , scores, num in zip(topic_words,word_scores,topic_nums):\n",
        "    print(num)\n",
        "    print(f\"words:{words}\")"
      ],
      "metadata": {
        "id": "tECDOuReytgq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df.head()"
      ],
      "metadata": {
        "id": "xRsMHOXQywqC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_df)"
      ],
      "metadata": {
        "id": "1rH5JA4Y7yCp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize new columns in the DataFrame\n",
        "train_df['dominant_topic'] = -1\n",
        "train_df['highest_topic_score'] = 0  # To store the highest similarity score\n",
        "\n",
        "num_topics = modeltop2vec.get_num_topics()\n",
        "topic_sizes = modeltop2vec.get_topic_sizes()[0]\n",
        "\n",
        "# Iterate through each topic\n",
        "for t in range(num_topics):\n",
        "    # Get the most relevant documents and their scores for the current topic\n",
        "    documents, document_scores, document_ids = modeltop2vec.search_documents_by_topic(topic_num=t, num_docs=topic_sizes[t])\n",
        "\n",
        "    for doc_index, doc_id in enumerate(document_ids):\n",
        "        # Ensure doc_id is in train_df\n",
        "        if doc_id in train_df.index:\n",
        "            # Check if this topic's score for the document is higher than the previously stored score\n",
        "            if document_scores[doc_index] > train_df.at[doc_id, 'highest_topic_score']:\n",
        "                # Update the dominant topic and highest topic score\n",
        "                train_df.at[doc_id, 'dominant_topic'] = t\n",
        "                train_df.at[doc_id, 'highest_topic_score'] = document_scores[doc_index]\n",
        "\n",
        "# Drop the 'highest_topic_score' column if it's no longer needed\n",
        "#train_df.drop('highest_topic_score', axis=1, inplace=True)\n",
        "\n"
      ],
      "metadata": {
        "id": "cSVlR7XByyca"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''# Initialize the column for the count of topic words\n",
        "train_df['count_of_topic_words_matching_clean_data'] = 0\n",
        "\n",
        "# Retrieve topic words for each topic\n",
        "topic_words_list = modeltop2vec.get_topics()[0]\n",
        "\n",
        "# Convert the list of words for each topic into a set for faster lookup\n",
        "topic_words_sets = [set(words) for words in topic_words_list]\n",
        "\n",
        "# Iterate over each row in the DataFrame\n",
        "for index, row in train_df.iterrows():\n",
        "    # Get the dominant topic for the current document\n",
        "    dominant_topic = row['dominant_topic']\n",
        "\n",
        "    # Continue only if a dominant topic is assigned\n",
        "    if dominant_topic != -1:\n",
        "        # Tokenize the 'clean_data' text into words\n",
        "        document_words = set(row['clean_data'].split())\n",
        "\n",
        "        # Count how many words are in common with the topic's words\n",
        "        common_words_count = len(document_words.intersection(topic_words_sets[dominant_topic]))\n",
        "\n",
        "        # Assign the count to the 'count_of_topic_words' column\n",
        "        train_df.at[index, 'count_of_topic_words_matching_clean_data'] = common_words_count\n",
        "\n",
        "# Now 'train_df' contains a new column 'count_of_topic_words' with the count of matching topic words for each document\n",
        "'''"
      ],
      "metadata": {
        "id": "P3WD3q8dD7Vc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df.info()"
      ],
      "metadata": {
        "id": "POSLErQ9k7FV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df.head(120)"
      ],
      "metadata": {
        "id": "Gn2bVuuhy8nY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Resetting the index of the DataFrame\n",
        "train_df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "# Saving the DataFrame to a Feather file\n",
        "train_df.to_csv('df_topicall_trump.csv')\n"
      ],
      "metadata": {
        "id": "Whi6Agcg1S-0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install seaborn"
      ],
      "metadata": {
        "id": "OkOXR5Av0_AC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Retrieve topics and their words\n",
        "topics_words,word_scores,topic_nums = modeltop2vec.get_topics()\n",
        "\n",
        "# Initialize an empty list to store the topic-word pairs\n",
        "topic_word_pairs = []\n",
        "\n",
        "# Iterate over each topic to get the words and create pairs\n",
        "for idx, topic_num in enumerate(topic_nums):\n",
        "    words = topics_words[idx]\n",
        "    topic_word_pairs.append({'Topic': topic_num, 'Word': words})\n",
        "\n",
        "# Convert the list of dictionaries to a DataFrame\n",
        "df_topic_word_pairs = pd.DataFrame(topic_word_pairs)\n",
        "\n",
        "# topic_words_df now contains two columns: 'Topic' and 'Word'\n"
      ],
      "metadata": {
        "id": "9OOPgArqzPBz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Resetting the index of the DataFrame\n",
        "df_topic_word_pairs.reset_index(drop=True, inplace=True)\n",
        "\n",
        "# Saving the DataFrame to a Feather file\n",
        "df_topic_word_pairs.to_feather('df_topic_list.csv')\n"
      ],
      "metadata": {
        "id": "jxVIjYDT1uDZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Analyzing the relationship between 'dominant_topic' and 'retweet_count'\n",
        "topic_retweet_analysis = train_df.groupby('dominant_topic')['retweet_count'].mean().sort_values(ascending=False)\n",
        "\n",
        "# Plotting the analysis\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(x=topic_retweet_analysis.index, y=topic_retweet_analysis.values)\n",
        "plt.title('Average Retweet Count by Dominant Topic')\n",
        "plt.xlabel('Dominant Topic')\n",
        "plt.ylabel('Average Retweet Count')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "RFLV5VcWuUeq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df.info()"
      ],
      "metadata": {
        "id": "LuEgWSlBLFtf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# to predict best topic with highest reach given a data"
      ],
      "metadata": {
        "id": "tMRFFOODkI07"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "regression -model"
      ],
      "metadata": {
        "id": "B5Kb9wSKFj6J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "\n",
        "# Assuming you have a DataFrame named 'train_df'\n",
        "# Features and target for the regression model\n",
        "features_reg = train_df[['day_of_week', 'month', 'day', 'hour', 'minute', 'second', 'year', 'tweet_length', 'sentiment']]\n",
        "target_reg = train_df['retweet_count']\n",
        "\n",
        "# Encoding categorical variables and standardizing numerical variables\n",
        "encoder = OneHotEncoder()\n",
        "scaler = StandardScaler()\n",
        "encoded_features_reg = encoder.fit_transform(features_reg[['day_of_week', 'month', 'sentiment']]).toarray()\n",
        "numerical_features_reg = scaler.fit_transform(features_reg[['day', 'hour', 'minute', 'second', 'year', 'tweet_length']])\n",
        "X_reg = np.hstack([encoded_features_reg, numerical_features_reg])\n",
        "\n",
        "# Splitting the dataset\n",
        "X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(X_reg, target_reg, test_size=0.2, random_state=42)\n",
        "\n",
        "# Training the Random Forest regression model\n",
        "random_forest_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "random_forest_model.fit(X_train_reg, y_train_reg)\n",
        "\n",
        "# Predicting and evaluating the regression model\n",
        "y_pred_reg = random_forest_model.predict(X_test_reg)\n",
        "mse_reg = mean_squared_error(y_test_reg, y_pred_reg)\n",
        "r2_reg = r2_score(y_test_reg, y_pred_reg)\n",
        "\n",
        "print('MSE:', mse_reg)\n",
        "print('R2 Score:', r2_reg)\n"
      ],
      "metadata": {
        "id": "jB88C2CKujai"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "for the regression we got this r2 beacuse we dont have the features that explains alot of the target varble which retweet count. we would have get a better resulst if we have features like number of likes , unlikes, followers  and commnets."
      ],
      "metadata": {
        "id": "Cxc5yqxOF0on"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "classifier -model -1"
      ],
      "metadata": {
        "id": "LVTvBGiQFm3C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Features and target for the classification model\n",
        "features_class = train_df[['day_of_week', 'month', 'day', 'hour', 'minute', 'second', 'year', 'tweet_length', 'sentiment','retweet_count']]\n",
        "target_class = train_df['dominant_topic']\n",
        "\n",
        "# Encoding and standardizing features for the classification model\n",
        "encoded_features_class = encoder.transform(features_class[['day_of_week', 'month', 'sentiment']]).toarray()\n",
        "numerical_features_class = scaler.transform(features_class[['day', 'hour', 'minute', 'second', 'year', 'tweet_length']])\n",
        "X_class = np.hstack([encoded_features_class, numerical_features_class])\n",
        "\n",
        "# Splitting the dataset for classification\n",
        "X_train_class, X_test_class, y_train_class, y_test_class = train_test_split(X_class, target_class, test_size=0.2, random_state=42)\n",
        "\n",
        "# Training the classification model\n",
        "classifier_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "classifier_model.fit(X_train_class, y_train_class)\n",
        "\n",
        "# Predicting and evaluating the classification model\n",
        "y_pred_class = classifier_model.predict(X_test_class)\n",
        "accuracy_class = (y_pred_class == y_test_class).mean()\n",
        "\n",
        "print('Accuracy:', accuracy_class)\n"
      ],
      "metadata": {
        "id": "TogkBIuVuy33"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score\n",
        "\n",
        "# Calculate the F1 score\n",
        "f1 = f1_score(y_test_class, y_pred_class, average='weighted')  # 'weighted' takes into account label imbalance\n",
        "\n",
        "print('F1 Score:', f1)\n"
      ],
      "metadata": {
        "id": "aMmGOD4A_e8Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# test"
      ],
      "metadata": {
        "id": "B7jtNtDXzvzE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_dff =test_df.copy()"
      ],
      "metadata": {
        "id": "3HKQbkB-zxyD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_dff.info()"
      ],
      "metadata": {
        "id": "JroE5eBI5msD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_df.rename(columns={'retweet_count': 'actual_retweet_count'}, inplace=True)"
      ],
      "metadata": {
        "id": "aSsktO5pyZnq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_df.info()"
      ],
      "metadata": {
        "id": "IaklbUzJwpYp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "features_test = test_df[['day_of_week', 'month', 'day', 'hour', 'minute', 'second', 'year', 'tweet_length', 'sentiment']]\n",
        "\n",
        "# Encoding and standardizing the test data\n",
        "# Note: Use the same encoder and scaler fitted on the training data\n",
        "encoded_features_test = encoder.transform(features_test[['day_of_week', 'month', 'sentiment']]).toarray()\n",
        "numerical_features_test = scaler.transform(features_test[['day', 'hour', 'minute', 'second', 'year', 'tweet_length']])\n",
        "X_test = np.hstack([encoded_features_test, numerical_features_test])\n",
        "\n",
        " #Making predictions on the test data\n",
        "predicted_retweet_count = random_forest_model.predict(X_test)"
      ],
      "metadata": {
        "id": "M8aW_mHdxHxa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Attach the predictions to the test dataset\n",
        "test_df['retweet_count'] = predicted_retweet_count\n",
        "\n",
        "# Now test_df contains an additional column with the predicted retweet counts\n",
        "test_df.head()  # Display the first few rows of the modified test dataset\n"
      ],
      "metadata": {
        "id": "lXKetrQexS9v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare features for the test data\n",
        "features_test = test_df[['day_of_week', 'month', 'day', 'hour', 'minute', 'second', 'year', 'tweet_length', 'sentiment','retweet_count']]\n",
        "# Encoding and standardizing the test data\n",
        "# Note: Use the same encoder and scaler fitted on the training data\n",
        "encoded_features_test = encoder.transform(features_test[['day_of_week', 'month', 'sentiment']]).toarray()\n",
        "numerical_features_test = scaler.transform(features_test[['day', 'hour', 'minute', 'second', 'year', 'tweet_length']])\n",
        "X_test = np.hstack([encoded_features_test, numerical_features_test])\n",
        "\n",
        "# Making predictions on the test data\n",
        "predicted_dominant_topic = classifier_model.predict(X_test)"
      ],
      "metadata": {
        "id": "h1Rss6aYxguZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Attach the predictions to the test dataset\n",
        "test_df['predicted_dominant_topic'] = predicted_dominant_topic\n",
        "\n",
        "# Now test_df contains an additional column with the predicted retweet counts\n",
        "test_df.head()  # Display the first few rows of the modified test dataset"
      ],
      "metadata": {
        "id": "_Fb2iDyEyFfP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# evaluation\n",
        "\n"
      ],
      "metadata": {
        "id": "g5vnvAWY0YXc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Assuming your dataset is loaded into a DataFrame named df\n",
        "\n",
        "# Calculate the difference between actual and predicted retweet counts\n",
        "test_df['difference'] = test_df['retweet_count'] - test_df['actual_retweet_count']\n",
        "\n",
        "# Calculate the total sum of differences (absolute value)\n",
        "total_difference = test_df['difference'].abs().sum()\n",
        "\n",
        "# Calculate the sum of actual retweet counts\n",
        "total_actual_retweet_count = test_df['actual_retweet_count'].sum()\n",
        "\n",
        "# Calculate the total percentage of error\n",
        "total_percentage_error = (total_difference / total_actual_retweet_count) * 100\n",
        "\n",
        "# Print the total difference and total percentage of error\n",
        "print(\"Total Percentage of Error:\", total_percentage_error)\n"
      ],
      "metadata": {
        "id": "WYydyUlZ0fsB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "even though we got the less r2 , the error percentage for the retweent count is 22% which means we got a better results form this model."
      ],
      "metadata": {
        "id": "o91k1fQxGiTr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Analyzing the relationship between 'dominant_topic' and 'retweet_count'\n",
        "topic_retweet_analysis = test_df.groupby('predicted_dominant_topic')['retweet_count'].mean().sort_values(ascending=False)\n",
        "\n",
        "# Plotting the analysis\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(x=topic_retweet_analysis.index, y=topic_retweet_analysis.values)\n",
        "plt.title('Average Retweet Count by Dominant Topic')\n",
        "plt.xlabel('Dominant Topic')\n",
        "plt.ylabel('Average Retweet Count')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "1ITc38JU1kFG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#to predict the topic given a tweet"
      ],
      "metadata": {
        "id": "DnjgFe46kAN3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "classifer_model-2"
      ],
      "metadata": {
        "id": "i63SYz6fFwa3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Feature Engineering\n",
        "features_selected = [\n",
        "    'vader_sentiment',\n",
        "    'day_of_week',\n",
        "    'month',\n",
        "    'tweet_length',\n",
        "    'retweet_count'\n",
        "]\n",
        "\n",
        "# Prepare the training data\n",
        "X_train_features = train_df[features_selected]\n",
        "y_train = train_df['dominant_topic']\n",
        "\n",
        "# Encoding categorical variables in training data\n",
        "day_encoder = LabelEncoder()\n",
        "month_encoder = LabelEncoder()\n",
        "X_train_features['day_of_week'] = day_encoder.fit_transform(X_train_features['day_of_week'])\n",
        "X_train_features['month'] = month_encoder.fit_transform(X_train_features['month'])\n",
        "\n",
        "# Normalizing the training features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train_features)\n",
        "\n",
        "# Splitting the training dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_train_scaled, y_train, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train the Random Forest classifier\n",
        "rf_model = RandomForestClassifier(random_state=42)\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n",
        "# Prepare the test data\n",
        "X_test_features = test_df[features_selected]\n",
        "X_test_features['day_of_week'] = day_encoder.transform(X_test_features['day_of_week'])\n",
        "X_test_features['month'] = month_encoder.transform(X_test_features['month'])\n",
        "\n",
        "# Normalizing the test features using the same scaler object\n",
        "X_test_scaled = scaler.transform(X_test_features)\n"
      ],
      "metadata": {
        "id": "qktuf_dibTzZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model Evaluation\n",
        "y_pred = rf_model.predict(X_test)\n",
        "report = classification_report(y_test, y_pred)\n",
        "\n",
        "report"
      ],
      "metadata": {
        "id": "NnRc82u7d2L-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculating the weighted average F1 score\n",
        "f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "\n",
        "print('Weighted Average F1 Score:', f1)"
      ],
      "metadata": {
        "id": "Gdqv0L7w_zsJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_test_pred = rf_model.predict(X_test_scaled)\n",
        "\n",
        "# Attach the predictions to the test dataset\n",
        "test_dff['predicted_dominant_topic_2'] = y_test_pred\n",
        "\n",
        "test_dff.head()  # Display the first few rows of the modified test dataset"
      ],
      "metadata": {
        "id": "tvJpf3QF5bqW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming 'tweetid' is a common identifier in both test_df and test_dff\n",
        "test_df = test_df.merge(test_dff[['tweetid', 'predicted_dominant_topic_2']], on='tweetid', how='left')\n",
        "\n",
        "# Display the first few rows of the modified test_df\n",
        "test_df.head()\n"
      ],
      "metadata": {
        "id": "7fVNdfNs3sUt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "for predictive analysis we done three models\n",
        "\n",
        "1.   Regression Model:\n",
        "\n",
        "Purpose: To predict the retweet count.\n",
        "Features Used: A set of features was experimented with, including dominant topic and sentiment score. However, to achieve the best R value, a refined set of features was selected.\n",
        "Performance: The model's performance was evaluated based on the R value, which measures the proportion of variance in the dependent variable that is predictable from the independent variables.\n",
        "2.   Classifier Model 1:\n",
        "\n",
        "Purpose: To predict the dominant topic.\n",
        "Features Used: 'day_of_week', 'month', 'day', 'hour', 'minute', 'second', 'year', 'tweet_length', 'sentiment', 'retweet_count'.\n",
        "Performance: This model achieved an accuracy of 92% and an F1 score of 89%, indicating its effectiveness in classifying the dominant topic based on the given features.\n",
        "3.   Classifier Model 2:\n",
        "\n",
        "Purpose: Also to predict the dominant topic.\n",
        "Features Used: 'vader_sentiment', 'day_of_week', 'month', 'tweet_length', 'retweet_count'.\n",
        "Performance: This model showed an accuracy of 93% and an F1 score of 90.10%, demonstrating a slightly improved performance compared to Classifier Model 1. even the clasifier still gave the best results it would have given much more better results if we more data as mentioned before or used larger dataset(we have to filter the data becuse of ur resurces limitations.)\n",
        "\n",
        "\n",
        "Direct Analysis: By examining the retweet count vs. dominant topic bar graph, stakeholders can identify which topics garner the most retweets and tailor their content accordingly.\n",
        "\n",
        "or\n",
        "Predictive Approach:\n",
        "we can predict the dominat topic in two ways by the available features.\n",
        "\n",
        "1. Time and Sentiment-Based Prediction: If only time data and sentiment (positive or negative) are available, stakeholders can first use the regression model to predict the retweet count [becuse it is part of the classifier model features], and then apply Classifier Model 1 to predict the most effective topic.\n",
        "\n",
        "2.  Text, Sentiment Score, and Time-Based Prediction: With access to the text, sentiment score, and time details, stakeholders can use the regression model to forecast retweet counts followed by Classifier Model 2 to determine the topic. This approach is suitable for stakeholders interested in both the potential reach (retweets) and the topic relevance of their content.\n",
        "\n",
        "3.  Direct Topic Prediction: Alternatively, stakeholders can directly use either Classifier Model 1 to predict the most suitable topic based on their requirements such as time and desired number of retweets(whichthey can assign them selfes), without incorporating the regression model in the process.\n",
        "\n",
        "Each approach offers different insights and applications, allowing stakeholders to choose the one that best fits their specific needs and available data.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "lm2s9s0BG0S9"
      }
    }
  ]
}